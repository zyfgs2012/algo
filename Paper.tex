\documentclass{llncs}
\bibliographystyle{splncs}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{indentfirst}
\usepackage[colorlinks,linkcolor=blue,anchorcolor=blue,citecolor=blue]{hyperref}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\renewcommand{\algorithmicrequire}{\textbf{Input:}} 
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{document}


\title{Passenger Prediction in Shared Accounts for Flight Service Recommendation}

%


\author{Yafeng Zhao, Jian Cao}
\institute{Department of Computer Science and Engineering,\\
Shanghai Jiao Tong University, Shanghai 200240, China\\
\email{zyfgs2012@163.com, cao-jian@sjtu.edu.cn}}


\maketitle
\begin{abstract}
Personalized recommendation is needed for online flight booking service because it is a difficult task for a traveller to select the flight when the number of available flights is large. Traditionally, we can recommend flights to a user based on his historical orders collected from his account. However, people sometimes book tickets for his family members, friends or colleagues through his account. In this case, the preferences of other travellers should also be considered. Unfortunately, before placing the order, people will not provider passengers' information. Therefore, we propose a probabilistic method for passenger prediction based on historical behaviors and contextual knowledge. We then experimentally demonstrate its effectiveness on a real dataset. The result shows that our method outperforms conventional methods.\\
\keywords{recommender system, probabilistic topic model, shared account}
\end{abstract}


\section{Introduction}
\label{sec:intro}
Recent years, with the rapid development of online travel agencies, there are more and more passengers booking flights through online travel service providers. Typically, an user may input his departure city, arrival city and departure time, then he will get a list of candidate flights. In general, there are dozens of flights within a list. We perform a search for flights from Beijing to Hong Kong at a website, there are total 156 matchable flights. The website only provides simple sorting strategies such as departure time, arrival time or price. Thus, it may take quite a long time for users to find the appropriate one. Therefore, a recommender system is necessary for better user experiences.

\par Essentially, a recommender system is used for generating a personalized ranking on a set of items\cite{lv:rec}. Typical recommender systems assume that the preference model of an individual can be learned from each account. However, for online flight booking, it is quite common that a user books flights for his family members, friends or colleagues.  Obviously, every passenger has his own preferences, which should be considered in flight recommendation. Unfortunately, before placing the order, people will not provider passengers' information.  Therefore, if we want to improve the accuracy of recommendation, we have to predict who are going to fly.  \par
To predict who is using the account to place orders is regarded as the shared account problem. Some approaches have been proposed to extract implicit user information within an account. For example,  in \cite{kabbur:nlmf}\cite{jason:embedding} the nonlinear latent factorization is applied to  extract user information. However, these methods only rely on the analysis of relationships between products and users \cite{koen:top-n}\cite{yutaka:modeling}, and context information (such as time information, the query input) that can help differentiate users is neglected. Besides, current methods always assume an account represents a fixed number of implicit users, which are not always true in reality. Moreover, so far, the experiments have only been evaluated on artificial composite datasets. \par
The flight booking service providers can acquire passenger's personal information after he has submitted the order. Thus, our goal is to predict distributions of passengers in current session based on historical behaviors and contextual information so that we can make flight recommendations in a more precise way. To achieve this target, we adopt the probabilistic author-topic model to analyze passengers' behaviors. We regard all historical orders in an account as the corpus, all passengers appearing in the account as authors and orders together with context information as documents.\par
In experiment section, we apply our method to a real flight order dataset. And the result shows that our method has higher identification accuracy and a better recommendation accuracy.\par
This paper has following contributions:\\
\begin{enumerate}
\item We provide a probabilistic passenger identification method for online flight booking service.
\item We apply the passenger prediction results to flight recommendation.
\item We evaluate our approach on a real flight order dataset, and the result shows that our approach can effectively improve the recommendation accuracy.
\end{enumerate}

\textbf{Paper organization.} The rest of paper is structured as follows. We introduce related work in Section \ref{sec:relw}. In Section \ref{sec:mod}, we define the problem and describe the model. Besides, we also discuss how to do parameter inference. In Section \ref{sec:rec}, we propose a preference-based flight ticket recommendation approach, which also plays a role as a baseline method at evaluation stage. In Section \ref{sec:exp}, we introduce the experiment dataset, evaluation metrics, the experimental results of passenger prediction and flight recommendation. Finally, we conclude the paper in Section \ref{sec:con}.

\section{Related Work}\par
\label{sec:relw}
\textbf{Content based Recommendation.} In online travel service industry such as airline tickets or hotel booking, the item (for example, an air ticket, a hotel booking order) information is usually highly structured, however, this information is quite mutable. Content-based recommender systems \cite{tech:cset}\cite{lops:handbook}are appropriate for highly structured items. The method needs proper representations of the attributes of items and the profile of user interests, then it tries to match the profile of user's interests with attributes of items.\par
\textbf{Flight Ticket Recommendation.} There is little work focusing on personalized flight  recommendations. Many online flight booking agencies provide sorting and filtering strategies to improve users' experiences, such as sorting by price and filtering by airline etc. These methods do make sense in helping users find appropriate flights within less time. However, they can not provide personalized recommendations. Flight recommendation mainly bases on implicit feedback\cite{song:mining}, which suffers from problems of data noises and lack of negative feedbacks. In 2004, Lorcan Coyle\cite{lor:flight} proposed a Personal Travel Assistant(PTA). PTA is based on case-based reasoning, which takes users' historical orders as cases and recommends the most similar candidate flights to users' queries.\par

\textbf{Probabilistic Topic Models.}  Topic models\cite{blei:lda}\cite{mic:atm}\cite{mark:prob} are proposed for automated extraction of useful semantic information from corpus data. The main step is extracting latent factors from the corpus, named topics, which are commonly probabilistic distributions over words. Topic models provide a completely unsupervised approach to extract topics, thus requiring no document labels and no initialization. Besides, each document may consist of multiple topics and it can be considered as probabilistic distributions over topics. The author-topic model is a generative model that extends LDA to include authorship information for document modeling. In our approach, the topic model is innovatively applied to identify users. \par

\textbf{Shared Account Recommendation.} Some work aims at addressing the challenge of identifying users who share a single account. A top-$N$ recommendation for shared account  was proposed by Koen et al\cite{koen:top-n}. It is an item-based top-$N$ collaborative filtering recommender system on binary and positive-only feedbacks. Another method proposed by Santosh et al.\cite{kabbur:nlmf} models users in an account with a much richer representation. It uses a nonlinear matrix factorization methods for predicting the recommendation score. Yutaka et al.\cite{yutaka:modeling} introduced an approach for modeling multiple users' purchase in a single account using an extended pLSA model. All above methods are applied in scenario that users' personal information can't be acquired explicitly.

\section{Passenger Prediction Model}
\label{sec:mod}
In this Section, we propose a generic model for predicting the probabilistic distribution of passengers, which can be used for recommendation.  Figure \ref{fig:over} shows an overview of the recommendation process. At first, a dataset of orders is saved into the database, in which each account may correspond to several passengers. We use this dataset to train the passenger prediction model as well as to extract passengers' preferences. Before a recommendation process starts, we calculate passenger distribution probability applying trained model and contextual information of current session. At the final step, we generate a ranked candidate list as the recommendation result based on passenger prediction and passengers' preference information.\par
\begin{figure}[!hbt]
\centering
\includegraphics[scale=.40]{overview.eps}
\caption{Overview of flight recommendation based on passenger prediction}
\label{fig:over}
\end{figure}\par

\subsection{Model description and notation}
Topic models are widely used in recommender systems. For example, in pLSA model\cite{tomas:coll}, a topic is a multinomial distribution over items and it represents a latent feature. An account is a multinomial distribution over topics and it can represent the preferences of an account. Each purchase can be regarded a sample that selects a topic $z$ from account $a$, and takes an item from topic $z$.\par
When the topic model is applied to flight recommendation, there are some challenges. Firstly, the concepts behind flight tickets are quite difficult to be modeled. Conventional items only have static features, which means that the contents of an item are less likely to change in the future. For flight tickets, however, the price changes frequently even for the same flight number and class. Since the price factor plays a significant role in users' choice, so we can't consider the tickets with different prices as different items. Secondly, the amount of tickets for a flight is limited, which leads to a very sparse user-item matrix so that we can't use a collaborative-filtering like method directly. Fortunately, the decisive factors of a flight ticket are quite fixed, such as airline, takeoff time, price and class etc. The number of alternatives for each factor is also quite limited.\par
Our notations are summarized in table \ref{tab:not}.\par
\begin{table}[!htbp]
\centering
\caption{\label{tab:not}Notation}
\begin{tabular}{|c|c|} \hline
$M$ & number of accounts within the dataset\\ \hline
$V$ & number of words in vocabulary\\ \hline
$O$ & orders in an account\\ \hline
$P$ & passengers in an account\\ \hline
$P_i$ & passengers for order $O_i$ \\ \hline
$F$ & selected factors\\ \hline
$V$ & size of vocabulary\\ \hline
$K$ & number of topics\\ \hline
\end{tabular}
\end{table}

In author-topic model\cite{deiv:corpora}, a predefined vocabulary is generated containing discrete alternatives and intervals of all selected factors. We treat every order as a bag of words, thus deduce each order to a vector of word counts. Each passenger is associated with a multinomial distribution over topics and each topic is associated with a multinomial distribution over entries.\par

For an account $M$, we generate the observed passenger list $P$. We denote passengers' distribution over topics by a $|P| \times K$ matrix $\Theta$. The multinomial distribution can be generated from Dirichlet prior distribution with hyper-parameter $\alpha$. Topics' distribution over words is denoted by a $K \times \sum_{i=1}^{|F|}|F_i| $  matrix $\Phi$, also, distribution can be generated with hyper-parameter $\beta$. Generally, these hyper-parameters needn't be estimated, here, we fix $\alpha$ and $\beta$ at 50/K and 0.01 respectively. \par

To generate a word, we need draw two latent variables,  a passenger and a topic, respectively. First we draw a passenger uniformly from $P$,  a topic $Z$ from $\Theta_A$ and a word $w$ from $\Phi_Z$. The following process describes the generative model mathematically.\par

\begin{enumerate}
\item For each passenger $p=1,\dots,|P|$ draw $\Theta_p \sim Dirichlet(\alpha)$
\item For each topic $t=1,\dots,K$ draw $\Phi_t \sim Dirichlet(\beta)$
\item For each order $o=1,\dots,O$
       \begin{enumerate}[fullwidth,itemindent=1em,label=(\alph*)]
       \item given passengers $P$
       \item For each word $i=1,\dots, N_o$
              \begin{enumerate}[fullwidth,itemindent=2em,label=(\roman*)]
              \item draw a passenger $X_{oi} \sim Uniform(P)$
              \item draw a topic $Z_{oi} \sim Discrete(\theta_{X_{oi}})$
              \item draw a word $w_{oi} \sim Discrete(\phi_{Z_{oi}})$
              \end{enumerate}
       \end{enumerate}
\end{enumerate}

\subsection{Parameter Inference}
As mentioned above, the author-topic model includes two sets of unknown parameters, the $P$ passenger-topic distributions $\theta$ and the $K$ topic-word distributions $\phi$. Generic EM algorithms are likely to face local maximum and computational inefficiency problems. Here we utilize Gibbs sampling\cite{gregor:esti} because it does not explicitly estimate parameters, instead, it evaluates posteriori distribution just based on drawn passengers $X$ and topics $Z$. Thus it is simple for Dirichlet priors.\par
We can obtain the probability of every word $\mathbf{w}_o$ generated in each order, conditioned on $\Theta$ and $\Phi$ is:\\
\begin{flalign}
\begin{split}
\label{eq:1} 
& P(\mathbf{w}_o | \Theta,\Phi,P) =\prod_{i=1}^{N_o}P(w_{oi}|\Theta,\Phi,\mathbf{p}_o) \\
& =\prod_{i=1}^{N_o}\sum_{p=1}^{|P|}\sum_{t=1}^{K}P(w_{oi}|z_{oi}=t,\Phi)
P(z_{oi}=t|x_{oi}=p,\Theta)P(x_{oi}=p|\mathbf{p}_o)\\
& =\prod_{i=1}^{N_o}\frac{1}{|P|}\sum_{p \in p_o}\sum_{t=1}^{K}\phi_{w_{oi}t}\theta_{tp}
\end{split} &
\end{flalign}

With the above generative model, $P(x_{oi}=p|\mathbf{p}_o)$ is assumed to be a uniform distribution over passenger list $P$. Each topic is drawn independently conditioned on $\Theta$ and $p_o$, and each word is drawn independently  conditioned on $\Phi$ and $z$. Equation \ref{eq:1} can be applied as the likelihood of all orders in a single account. If we treat $\Theta$ and $\Phi$ as random variables, our target is to estimate the Maximum A Posteriori for the generative model.\par
In Gibbs sampling process, in order to draw a sample from the joint distribution $P(\mathbf{z},\mathbf{x}|\alpha,\beta)$, we need to draw the assignment of passenger $x_{di}$ and topic $z_{di}$ for a word $w_{di}$ conditioned on previous assignments for all other words in the whole corpus. In general, every word in the corpus should be sampled, and the batch sampling process will be performed for several iterations. A Markov chain \cite{gregor:esti} can be constructed that converges to the posteriori distribution on passenger $x$ and topic $z$. $p(\Theta,\Phi|\mathbf{z},\mathbf{x},\alpha,\beta)$ can be calculated in terms of the property that Dirichlet distribution is conjugate to the multinomial distribution. Each pair of passenger and topic $(z_i,x_i)$ is drawn according to the following equation:\\
\begin{flalign}
\begin{split}
\label{eq:2}
& P(x_{oi}=p,z_{oi}=t|w_{oi}=w,\mathbf{z}_{-oi},\mathbf{x}_{-oi},\mathbf{w}_{-oi},\alpha,\beta,p_o)\\
&  \propto \frac{C_{tp}^{TP}+\alpha}{\sum{t'}C_{t'p}^{TP}+T\alpha}\frac{C_{wt}^{WT}+\beta}{\sum_{w'}C_{w't}^{WT}+W\beta}
\end{split} &
\end{flalign}\\
Equation \ref{eq:2} represents the probability of assigning topic $t$ and passenger $p$ for $i$-th word in order $o$. $C^{WT}$ is the word-topic matrix, and $C_{wt}$ is the times that word $w$ is assigned to topic $t$ except for the current word. $C^{TP}$ represents the topic-passenger matrix, and $C_{tp}$ indicates the times that passenger $p$ is assigned to topic $t$ except for the current word $w_{oi}$. Moreover, $W$ is the size of vocabulary, $T$ represents the number of topics and $P$ is the number of passengers. From the sampling metric, we can estimate the topic-word distribution and passenger-topic distribution:\\
\begin{equation}
\label{eq:3}
\theta_{tp} = \frac{C_{tp}^{TP}+\alpha}{\sum{t'}C_{t'p}^{TP}+T\alpha}
\end{equation}
\begin{equation}
\label{eq:4}
\phi_{wt} = \frac{C_{wt}^{WT}+\beta}{\sum_{w'}C_{w't}^{WT}+W\beta}
\end{equation}\\
where $\theta_{tp}$ is the probability of drawing topic $t$ conditioned on passenger $p$ and $\phi_{wt}$ is the probability of drawing word $w$ conditioned on topic $t$. Thus in the process of parameter inference, we need to keep matrix $C^{TP}$ and $C^{WT}$, besides, the sampled word-topic list $T$ and sampled word-passenger list $P$ should be updated after each sampling, where $T[o][i]$ represents the topic sampled for word $i$ in order $o$ and $P[o][i]$ represents the passenger sampled.\par
The algorithm performs in three steps, i.e., initialization, sampling and updating respectively. At the first step, we assign each word in the corpus with random passengers and topics. For every sampling operation, a word in corpus is chosen, the probability of topic distribution and passenger distribution is calculated conditioned on the rest words in corpus by applying Equation \ref{eq:2}, with these two probabilities, we can sample a new topic and passenger for the current word. After several batch iterations, the passenger-topic matrix $\Theta$ and topic-word matrix $\Phi$ can be updated by applying Equation \ref{eq:3}, \ref{eq:4}. So the computational efficiency is the number of words multiplies number of topics, passengers and iteration times.\par
\subsection{Passenger Prediction}
Given the passenger topic probability matrix $\Theta$ and topic-word probability matrix $\Phi$. We can predict passengers for an anonymous order. It's essentially a classification of an unlabeled order\cite{shanshan:unpopular}. We perform the classification by choosing the passenger whose apperance can maximize the probability $p(p|o_n)$, represented in the following equation:\\
\begin{equation}
\label{eq:5}
p(x=p|o_n,\Theta,\Phi) \propto p(p)\prod_{w \in o_n}\sum_t p(t|w)p(w|t,p)
\end{equation}\\
where $p(p) = |O_p| / |O|$, $|O_p|$ is the number of orders participated by passenger $p$. $p(w|t,p) = p(t|p) \times p(w|t)$ since the process of drawing topic from a passenger and drawing a word from a topic are independent of each other. $p(t|w)$ represents the probability that word $w$ is assigned to topic $t$, which can be computed by: $\frac{C^{wt}}{\sum_w'C^{w't}}$.\par
In conclusion, the passenger prediction task can be partitioned into two steps. For the first step, a set of decisive factors are extracted based on domain  knowledge to generate a vocabulary. And the parameter $\Theta$ and $\Phi$ are trained through several batches of Gibbs sampling. In the second step, passengers of an anonymous order can be predicted by applying Equation \ref{eq:5}. Algorithm \ref{alg:1} describes the predicting process mathematically.\par

\begin{algorithm}[htb]
\caption{passengerPrediction}
\begin{algorithmic}[1]
\label{alg:1}
\REQUIRE
Account history order list $O$. \par
Predefined discrete factor list $F$ \par
A test anonymous order $o$
\ENSURE 
User's prediction list $Pl$
\STATE List $Pl \leftarrow \emptyset$;
\STATE stat word vector $\mathbf{W}$ for all orders;
\STATE Model $M \leftarrow trainATM(W)$;
\STATE calculate user's probability by Equation \ref{eq:5};
\FORALL{user : P}
\IF{$user.probability > \frac{1}{|P|}$}
\STATE $Pl.append(user,probability)$;
\ENDIF
\ENDFOR
\STATE normalize $Pl$;
\RETURN $Pl$;
\end{algorithmic} 
\end{algorithm}

\section{The Preference-based recommendation approach}
\label{sec:rec}
There is little existing work aiming at providing flight recommendation. In this section, we propose a user preference-based recommendation approach. In Section \ref{sec:exp}, we evaluate this method against some ranking strategies, such as price rank etc, to demonstrate that this method has a better accuracy. Then, we will apply the passenger prediction results to improve recommendation accuracy further.

\subsection{Factor selection and frequency statistic}
A flight ticket contains a set of attributes. These attributes may be either discrete or continuous variables. We divide continuous attribute into a list of intervals, for example, takeoff time can be divided into morning, noon, afternoon, night etc. Thus, any factor can be represented by finite alternatives. Then, we count the distribution of alternatives from a user's historical orders to analyze user's preference on corresponding factor. In a word, the preference model can be described as a union of vectors, a vector indicates one factor and an element within one vector represents the frequency of corresponding alternative which the user has chosen.\par
We can also represent a candidate flight ticket as a vector, where each element is the corresponding alternative of each factor. So, for every ticket in candidate list, we compare the vector with users' preference model and get a score for every factor based on the frequency. We can sum up score of every factor to get a total score for that item, and rank all candidates according to the total score. We can then generate a top-$N$ recommendation.\par
\subsection{Weight of factors}
Since users may care different factors, we introduce personalized weight vector for each user. We estimate how much a user focuses on a factor through the information entropy, as mentioned in \cite{yang:pers}. We believe that if a user focuses on a factor, his behavior will be more concentrated. The entropy $H$ is a measure to describe the uncertainty for discrete random variable $X$ and probability mass function $P(X)$. Here is the mathematical representation for entropy.\\
\begin{equation}
H(X)=E[-lnP(X)]= - \sum_{i-1}^n P(x_i)log_bP(X_i)
\end{equation}\par
The value $b$ is the base of logarithm, and is usually 2. The entropy of every factor can be calculated and initialized as the weight value after normalization. \par
The preference based recommendation algorithm is described as Algorithm \ref{alg:3}. First extracting a user's preference  $P$ and initializing weighted list $W$ through entropy. Items in candidate list are ranked based on inner product of $W$ and corresponding value of factor.

\begin{algorithm}[htb]
\caption{Preference-Based-Recommendation}
\begin{algorithmic}[1]
\label{alg:3}
\REQUIRE
User's ID $u$ \par
User's history order list $O$ \par
Candidate item list $C$
\ENSURE Ranked candidate list $R$
\STATE Define a factor list $F$;
\STATE $P \leftarrow extractUserPref(O,F)$;
\STATE initialize $W$ by entropy;
\STATE $R \leftarrow \emptyset$;
\FORALL { $c : C$} 
\STATE append $(c,weightedScore(c,W))$ to R;
\ENDFOR 
\STATE sort $R$ by score;
\RETURN $R$;
\end{algorithmic} 
\end{algorithm}


\section{Experiments}
\label{sec:exp}
In the following subsections, we introduce datasets and evaluation metrics. Then we evaluate the performance of passenger prediction and passenger prediction based recommendation respectively.
\subsection{Dataset}
In our experiments, we use a real flight ticket dataset, consisting of all submitted orders for two years. An order contains account id and all passengers' id. We use desensitized passenger id to distinguish individuals from each other. In our experiment, we select active users whose amount of historical orders reaches a threshold, thus passengers' preferences can be extracted with more confidence. In addition, many researches\cite{amy:guess}\cite{yutaka:modeling} on shared account recommendation generate an artificial dataset by composing single accounts' data. We also generate an artificial dataset combining single passengers' data from two different accounts. The overviews of these two datasets and the meta data of orders are summarized in table \ref{tab:dat} and table \ref{tab:mdat}.
\begin{table}[!htbp]
\centering
\caption{\label{tab:dat}Datasets}
\begin{tabular}{|c|c|c|c|} \hline
& \# accounts & \# passengers & \# orders\\ \hline
Real & 4632&7034&38907 \\ \hline
Artificial &1604&3208&29759\\ \hline
\end{tabular}
\end{table}

\begin{table}[!htbp]
\centering
\caption{\label{tab:mdat}Meta data}
\begin{tabular}{|c|c|} \hline
Travel Info & \tabincell{c}{airline, departure city, arrival city, \\departure airport, arrival airport, \\takeoff time, arrival time}\\ \hline
Content Info & \tabincell{c}{order time, login ip, geo location,\\ other trace log information}\\ \hline
Individual Info & account-id, passenger-id, age, gender\\ \hline
Order Info & \tabincell{c}{flight number, price , craft type, \\class, rescheduling/canceling policy}\\ \hline
\end{tabular}
\end{table}

\subsection{Settings and Evaluation Metrics}
We take the latest order of a user as the test data and the rest ones as the training data. The test data also plays a role of providing contextual information for user prediction.\par
One significant issue is that we can't know real candidates at the time when a user performs the search action. Fortunately, we can get an approximate set of candidates by collecting orders from all users in the dataset with the same order date, takeoff date, departure city and arrival city as the test order. In order to get a convincing evaluation result, we filter out test orders with the size of candidate list less than 20 and the average size of candidates is 45.\par
We apply two commonly used metrics MAP(Mean Average Precision) and top-$N$ hit rate to evaluate the recommendation performance. The definition of MAP is described as follow:\\
\begin{equation}
MAP = \frac{\sum_{i=1}^{|M|}Acc(u_i)}{|M|}
\end{equation}

\begin{equation}
Acc(u_i) =
\begin{cases}
1 & \mbox{if N =1}\\
1 - \frac{index-1}{N-1} & \mbox{if N >1}
\end{cases}
\end{equation}

\begin{equation}
top-N = \frac{\sum_{i=1}^{|M|}|top-N(u_i) \cap O_{u_i}|}{|M|}
\end{equation}

where $|M|$ is the number of total test orders, $Acc(u_i)$ represents the accuracy of recommendation based on the rank percentages of test orders. The metric MAP is the average of each user's accuracy, and is a measurement for general performance. The top-$N$ hit rate is the actual strategy for recommendation, we can stick up top-$N$ candidates based on the ranked list. For each purchase, if top-$N$ candidates hit the chosen order, we consider this recommendation has accuracy 1, else the accuracy is 0. We also calculate MAP for all accounts \par
For passenger prediction, we propose the following metric to evaluate the accuracy:\\
\begin{equation}
\label{eq:10}
Acc(u_i) = \frac{P' \cap A}{\sqrt{|P'| \times |A|}}
\end{equation}\par
Since we treat every account as a corpus, so a probabilistic distribution list is generated for all passengers contained in the account. At the first stage, we need to decide the passenger number for each test order. Assuming that there are total $|P|$ passengers in an account. So in average case, the probability of every passenger is $P_A = \frac{1}{|P|}$. We decide the passenger number by omitting passengers whose probability is not greater than $P_A$. We denote predicted passenger list as $P'$, actual passenger list as $A$. Thus we can get the prediction accuracy by Equation \ref{eq:10}. The numerator is the number of hit passengers, the denominator is penalty item for the size of predicted passenger list and predicted passenger list.

\subsection{Passenger Prediction}
\label{sub:pred}
We train a probabilistic model for every account where topic number selected varying ${5,10,\dots,50}$. We pick a test order for each account as a new document, then predict every passenger's probability of placing this order by Equation \ref{eq:5}. According to above evaluating metrics, the mean prediction accuracy rate of (a) real dataset and (b) artificial dataset is shown in Figure \ref{fig:pred}. The left sub-figure is the evaluation result on the real dataset and the right one is on the artificial dataset. The figure's horizontal axis shows the number of trained topics and the vertical axis shows the prediction accuracy computed by applying above mentioned metrics. The Random method means assigning random probabilities to passengers, we also record $10$ batch of random results as our approach does. For the artificial dataset, there are two passengers in every account and one passenger for every test order, so the accuracy of prediction is either 1.0 or 0. The results demonstrate that our model has a higher prediction precision than the random method on both datasets. Besides, when the topic number is 10 or 15, both datasets get the best performance while the performance goes down with the topic number increasing, which demonstrates that the size of corpus' vocabulary and number of words in each order also impact the optimal topic number. In the next subsection, we evaluate the recommendation performance with topic number fixed at 10.\par 

\begin{figure}[!h]
              \begin{minipage}[t]{0.47\linewidth}
              \centering
              \includegraphics[width=1.145\linewidth]{e5.eps}\\
              \end{minipage}
              \begin{minipage}[t]{0.005\linewidth}~~~
              \end{minipage}
              \begin{minipage}[t]{0.47\linewidth}
              \centering
              \includegraphics[width=1.145\linewidth]{e6.eps}\\
              \end{minipage}
              \begin{minipage}[t]{0.005\linewidth}~~~
              \end{minipage}
              \caption{MAP of user prediction}
          \label{fig:pred}
\end{figure}


\subsection{Recommendation}
After obtaining the predicted probabilistic distribution of passengers, we can extend preference based recommendation approach by extracting preferences of predicted passengers. If there are multiple predicted passengers for one test order, we generate a composite preference model that combines all involved passenger's preferences together. \par
We use MAP and top-$N$ hit rate mentioned above to evaluate recommendation performance. We compare different recommendation approaches including preference based recommendation(shorted as Pre), transfered preference based recommendation(tfPre), price rank(pRank), hot rank(hRank) and predicting preference based recommendation(pdPre). The transfered preference based method is the same as the previous one except it takes the difference between flight distribution of various air routes into consideration. This method groups training data by route, and evaluates similarity between routes by flight distribution on some attributes such as airline, class and price level. If a passenger isn't active on the target route, we transfer his preferences from most similar active routes with a transfer rate. In our experiment, the transfer rate is  $ \alpha = 0.5$. Price rank is a simple strategy that ranks candidates based on price in an ascending order. Hot rank is a strategy that ranks candidates according to the popularities, which is based on total order amount within a period of previous two weeks. \par

\begin{figure}[!h]
              \begin{minipage}[t]{0.47\linewidth}
              \centering
              \includegraphics[width=1.145\linewidth]{e1.eps}\\
              \end{minipage}
              \begin{minipage}[t]{0.005\linewidth}~~~
              \end{minipage}
              \begin{minipage}[t]{0.47\linewidth}
              \centering
              \includegraphics[width=1.145\linewidth]{e2.eps}\\
              \end{minipage}
              \begin{minipage}[t]{0.005\linewidth}~~~
              \end{minipage}
              \caption{top-N accuracy of recommendation}
          \label{fig:top}
\end{figure}
\begin{figure}[!h]
              \begin{minipage}[t]{0.47\linewidth}
              \centering
              \includegraphics[width=1.145\linewidth]{e3.eps}\\
              \end{minipage}
              \begin{minipage}[t]{0.005\linewidth}~~~
              \end{minipage}
              \begin{minipage}[t]{0.47\linewidth}
              \centering
              \includegraphics[width=1.145\linewidth]{e4.eps}\\
              \end{minipage}
              \begin{minipage}[t]{0.005\linewidth}~~~
              \end{minipage}
              \caption{MAP of recommendation}
          \label{fig:rank}
\end{figure}\par
Figure \ref{fig:top} shows recommendation accuracy on the real dataset and artificial dataset respectively. The figure's horizontal axis shows the number of top-$N$ recommended flights. The accuracy increases linearly with the growth of recommended flights. Figure \ref{fig:rank} shows MAP on two datasets. The results show that the baseline recommendation approach performs much better than price rank strategy. And content based recommendation combined with passenger prediction can achieve a higher accuracy than baseline method. We can also notice that the improvements on the artificial dataset is larger than on the real dataset. The reason may be that passengers in artificial dataset have more diversified preferences since they are randomly selected and composed.

\section{Conclusion and future work}
\label{sec:con}
In this paper, we proposed a generic probabilistic model to predict passengers in a single flight booking account based on passengers' historical submitted orders. This model is appropriate for scenarios that individuals can not be explicitly identified before placing the order, such as online hotel booking. For specified contextual environment, we can make a prediction for passenger distribution. Then we integrate passenger prediction into recommendation process. For experiment evaluation, we propose a general preference based recommendation approach for implicit feedback, and verify the efficiency of proposed model on two datasets. The results suggest that both passenger prediction and recommendation approach achieve a higher accuracy. \par
In future work, we plan to do more research on how to determine the optimal amount of users in each session. 

\begin{thebibliography}{1}
\bibitem {koen:top-n}
Koen,V.,Bart,G.:
Top-N Recommendation for Shared Accounts.
In: Proceedings of the 9th ACM Conference on Recommender Systems,59–66(2015)

\bibitem {jason:embedding}
Jason,W.,Ron,J.,Hector,Y.:
Nonlinear Latent Factorization by Embedding Multiple User Interests.
In: Proceedings of the 7th ACM Conference on Recommender Systems,65-68(2013)

\bibitem {kabbur:nlmf}
Santosh,K.,George,K.:
NLMF: NonLinear Matrix Factorization Methods for Top-N Recommender Systems.
In: International Conference on Data Mining Workshop,167-174(2014)

\bibitem {amy:guess}
Amy,Z.,Nadia,F.,Stratis,I.:
Guess Who Rated This Movie: Identifying Users Through Subspace Clustering.
In: Proceedings of Conference on Uncertainty in Artificial Intelligence,944-953(2012)

\bibitem {yutaka:modeling}
Yutaka,K.,Tomoharu,I.,Ko,F.:
Modeling Multiple Users' Purchase over a Single Account for Collaborative Filtering.
In: Proceedings of the 11th International Conference on WISE,328-341(2010)

\bibitem {shanshan:unpopular}
Shanshan,F.,Jian,C.,Yuwen,C.,Jing,Q.:
A Model for Discovering Unpopular Research Interests.
In: Proceedings of the 8th International Conference KSEM,382-393(2015)

\bibitem {lops:handbook}
Pasquale,L.,Marco,G.,Giovanni,S.:
Content-based Recommender Systems: State of the Art and Trends.
In: Recommender Systems Handbook,Springer US,73-105(2011)

\bibitem {song:mining}
Yan,S.,Ping,Y.,Chunhong,Z.,Yang J.:
Implicit Feedback Mining for Recommendation.
In: Proceedings of the First International Conference on Big Data Computing and Communication,373-385(2015)

\bibitem {tech:cset}
Deivendran,T.,B,S.:
Content Based Recommender Systems.
In: International Journal of Computer Science and Emerging Technologies,382-393(2011)

\bibitem {blei:lda}
DM,B.,AY,N.,MI,J.:
Guess Who Rated This Movie: Identifying Users Through Subspace Clustering.
In: Journal of Machine Learning Research,993-1022(2003)

\bibitem {mic:atm}
Michal,R.,Thomas,G.,Mark,S.,Padhraic,S.:
The author-topic model for authors and documents.
In: Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence,487-494(2012)

\bibitem {mark:prob}
Mark,S.,Padhraic,S.,Michal,R.,Thomas,G.:
Probabilistic Author topic Models for Information Discovery.
In: Proceedings of ACM SigKDD conference knowledge discovery and data mining,306-315(2004)

\bibitem {deiv:corpora}
M,Rosen,Z.,Thomas,G.:
Learning author-topic models from text corpora.
In: ACM Transactions on Information Systems,312-324(2010)

\bibitem {gregor:esti}
Gregor,H.:
Parameter Estimation for Text Analysis.
University of Leipzig,Germany,2009

\bibitem {lv:rec}
Lu,L.,Matus,M.,Yeung,H.,Zhang,Y.:
Recommender systems.
Hangzhou Normal University,2012

\bibitem {tomas:coll}
Thomas,H.:
Collaborative filtering via gaussian probabilistic latent semantic analysis.
In: Proceedings of the 26th International ACM conference on Research and development in information retrieval,259-266(2004)

\bibitem {lor:flight}
Lorcan,C.:
Making Personalised Flight Recommendations using Implicit Feedback.
University of Dublin,2004

\bibitem {yang:pers}
Yang,F.:
Personalized Flight Recommender.
Shanghai Jiao Tong University,2016

\end{thebibliography}

\end{document}